{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMce8muBqXQP"
   },
   "source": [
    "# Tensorflow with GPU\n",
    "\n",
    "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM_8ELnJq_wd"
   },
   "source": [
    "## Enabling and testing the GPU\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "Next, we'll confirm that we can connect to the GPU with tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXnDmXR7RDr2",
    "outputId": "7808a8b6-c564-479c-87b3-8c77b7dbed79"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3fE7KmKRDsH"
   },
   "source": [
    "## Observe TensorFlow speedup on GPU relative to CPU\n",
    "\n",
    "This example constructs a typical convolutional neural network layer over a\n",
    "random image and manually places the resulting ops on either the CPU or the GPU\n",
    "to compare execution speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y04m-jvKRDsJ",
    "outputId": "2cc90f2a-6bad-4912-e4ef-7bb989f656a5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "def cpu():\n",
    "  with tf.device('/cpu:0'):\n",
    "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "\n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhWijohCWbyl"
   },
   "outputs": [],
   "source": [
    "#!pip install numba cupy-cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxflMFuxW1P-",
    "outputId": "d0f8d68b-e804-42fe-db3e-738ac5bcb710"
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    tx = cuda.threadIdx.x # Thread id in a block\n",
    "    ty = cuda.blockIdx.x  # Block id in a grid\n",
    "    bw = cuda.blockDim.x  # Block width, i.e. number of threads per block\n",
    "    pos = tx + ty * bw    # Compute flattened index inside the array\n",
    "\n",
    "    if pos < out.size:  # Check array boundaries\n",
    "        out[pos] = x[pos] + y[pos]\n",
    "\n",
    "# Host code\n",
    "n = 1000\n",
    "x = np.arange(n).astype(np.float32)\n",
    "y = 2 * x\n",
    "out = np.zeros_like(x)\n",
    "\n",
    "# Define thread hierarchy\n",
    "threads_per_block = 128\n",
    "blocks_per_grid = math.ceil(n / threads_per_block)\n",
    "\n",
    "# Start the kernel\n",
    "add_kernel[blocks_per_grid, threads_per_block](x, y, out)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wO8K1XFOW_bW",
    "outputId": "367ff9dc-20d3-492e-925d-626c6456b96b"
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "x_gpu = cp.array([1, 2, 3, 4, 5])\n",
    "y_gpu = cp.array([6, 7, 8, 9, 10])\n",
    "\n",
    "# Perform an element-wise addition on GPU\n",
    "z_gpu = x_gpu + y_gpu\n",
    "\n",
    "# Bring the result back to CPU memory\n",
    "z = z_gpu.get()\n",
    "\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fYRZ1YbXOIF",
    "outputId": "574cf5af-4140-4adc-8db2-2ee9f45b96af"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create large arrays\n",
    "size = 300000000  # 300 million elements\n",
    "x = np.random.rand(size)\n",
    "y = np.random.rand(size)\n",
    "\n",
    "# Element-wise operation on CPU\n",
    "start_time = time.time()\n",
    "z = x + y + x*x  # This operation is performed on the CPU\n",
    "end_time = time.time()\n",
    "\n",
    "cpu_time = end_time - start_time\n",
    "print(f\"Time taken on CPU: {cpu_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6tFaB-QXbJu",
    "outputId": "37f2e9da-fbd6-40e7-9cc9-3ac6d24b1a07"
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# Create large arrays\n",
    "x_gpu = cp.random.rand(size)\n",
    "y_gpu = cp.random.rand(size)\n",
    "\n",
    "# Element-wise operation on GPU\n",
    "start_time = time.time()\n",
    "z_gpu = x_gpu + y_gpu  + x_gpu*x_gpu # This operation is performed on the GPU\n",
    "cp.cuda.Stream.null.synchronize()  # Ensure the operation is complete\n",
    "end_time = time.time()\n",
    "\n",
    "gpu_time = end_time - start_time\n",
    "print(f\"Time taken on GPU: {gpu_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzRDP4FUXk6G",
    "outputId": "c9ee9e33-1703-4431-e1d7-0d2db8671217"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize matrices\n",
    "size = 10000  # size of square matrix\n",
    "A = np.random.rand(size, size)\n",
    "B = np.random.rand(size, size)\n",
    "\n",
    "# Perform matrix multiplication on CPU\n",
    "start_time_cpu = time.time()\n",
    "C = np.matmul(A, B)\n",
    "end_time_cpu = time.time()\n",
    "\n",
    "cpu_time = end_time_cpu - start_time_cpu\n",
    "print(f\"Time for matrix multiplication on CPU: {cpu_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7gZXXqDlYGHW",
    "outputId": "4a82431e-55a7-4d3a-a8c4-eca9fb99e8f6"
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# Initialize matrices\n",
    "A_gpu = cp.random.rand(size, size)\n",
    "B_gpu = cp.random.rand(size, size)\n",
    "\n",
    "# Perform matrix multiplication on GPU\n",
    "start_time_gpu = time.time()\n",
    "C_gpu = cp.matmul(A_gpu, B_gpu)\n",
    "cp.cuda.Stream.null.synchronize()  # Make sure the current stream is done with all operations\n",
    "end_time_gpu = time.time()\n",
    "\n",
    "gpu_time = end_time_gpu - start_time_gpu\n",
    "print(f\"Time for matrix multiplication on GPU: {gpu_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAzdNSc_lfnf",
    "outputId": "bef379c3-8772-4148-e87b-4aef75c5a737"
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "# Define a CUDA kernel function.\n",
    "# A kernel function is a GPU function that is meant to be called from CPU (host) code.\n",
    "# It's executed N times in parallel by N different CUDA threads, unlike usual functions.\n",
    "@cuda.jit\n",
    "def thread_block_grid_demo(output):\n",
    "    # cuda.threadIdx.x: This is the unique thread ID within each block (local ID).\n",
    "    # The x denotes that we're using a 1D block of threads, hence we're interested in the x-dimension ID.\n",
    "    tx = cuda.threadIdx.x\n",
    "\n",
    "    # cuda.blockIdx.x: This is the unique block ID within the grid of blocks launched by this kernel (also local ID).\n",
    "    # Like threadIdx, the x indicates we're using a 1D grid of blocks.\n",
    "    ty = cuda.blockIdx.x\n",
    "\n",
    "    # cuda.blockDim.x: This represents the number of threads in the block.\n",
    "    # This is constant for all threads and is set during the kernel launch.\n",
    "    bw = cuda.blockDim.x\n",
    "\n",
    "    # cuda.gridDim.x: This represents the number of blocks in the grid.\n",
    "    # Also constant for all threads and is set during the kernel launch.\n",
    "    grid_size = cuda.gridDim.x\n",
    "\n",
    "    # Calculate the unique position for this thread in the entire grid of threads.\n",
    "    # Each block computes a contiguous chunk of elements in the array.\n",
    "    pos = tx + ty * bw\n",
    "\n",
    "    # Boundary check: we don't want to go past the end of the array.\n",
    "    # This can happen if the total size isn't perfectly divisible by the block size.\n",
    "    if pos < output.size:\n",
    "        # Encoding the IDs in the output just for demonstration.\n",
    "        # This isn't typically done in production code.\n",
    "        # This operation uniquely combines the IDs for demonstration purposes.\n",
    "        output[pos] = tx + ty * 10 + bw * 100 + grid_size * 1000\n",
    "\n",
    "# The total number of elements that we want to compute.\n",
    "n = 60\n",
    "\n",
    "# The number of threads in each block.\n",
    "threads_per_block = 5\n",
    "\n",
    "# Calculate the number of blocks we need in our grid.\n",
    "# We divide the total size by the block size.\n",
    "# The addition and subtraction ensure we round up to account for all elements.\n",
    "blocks_per_grid = (n + (threads_per_block - 1)) // threads_per_block\n",
    "\n",
    "print(blocks_per_grid)\n",
    "\n",
    "# The output array where results will be stored.\n",
    "# The dtype is float32 because CUDA is most efficient with 32-bit data types.\n",
    "output_array = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "# Kernel launch.\n",
    "# The \"[blocks_per_grid, threads_per_block]\" specifies the grid and block dimensions.\n",
    "# \"output_array\" is the argument passed to the kernel function.\n",
    "thread_block_grid_demo[blocks_per_grid, threads_per_block](output_array)\n",
    "\n",
    "# After this line, the kernel has finished executing and the output_array on the host\n",
    "# (CPU memory) has been updated with whatever the kernel wrote into it on the device (GPU memory).\n",
    "\n",
    "print(output_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tslXZ8BlgH8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
