{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4941058a-6c14-4330-8dee-a9e137c516f3",
   "metadata": {},
   "source": [
    "## **Notebook 1: Introduction to CuPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb079367-934a-4d8e-a5c3-ec5592ac69ae",
   "metadata": {},
   "source": [
    "**Introduction to Workshop Lab Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe05310-7998-4a4f-a660-f762f3359fab",
   "metadata": {},
   "source": [
    "Markdown cells contain plain text, and code cells contain interactive Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d06d849-08b0-4dfc-b76d-b60be5b172cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f80bd-c004-4d16-9f1b-974c971d2235",
   "metadata": {},
   "source": [
    "We can also run shell commands by prepending an exclamation mark to our code cells. Let's query for some basic information about our system. `nvidia-smi` is like `top` for NVIDIA GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b74837c5-d1ed-4a7f-ba8c-a0d99c668093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov  6 17:54:10 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   22C    P8             21W /  300W |     278MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               On  |   00000000:23:00.0 Off |                  Off |\n",
      "| 30%   22C    P8             15W /  300W |     278MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               On  |   00000000:41:00.0 Off |                  Off |\n",
      "| 30%   21C    P8             18W /  300W |     703MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000               On  |   00000000:61:00.0 Off |                  Off |\n",
      "| 30%   21C    P8             26W /  300W |     278MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000               On  |   00000000:81:00.0 Off |                  Off |\n",
      "| 30%   23C    P8             32W /  300W |     278MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000               On  |   00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   21C    P8             14W /  300W |     278MiB /  49140MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000               On  |   00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   21C    P8             21W /  300W |     278MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000               On  |   00000000:E1:00.0 Off |                  Off |\n",
      "| 30%   22C    P8             22W /  300W |     278MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    0   N/A  N/A   1255129      C   ...IOE488/FA24/conda/rapids/bin/python        262MiB |\n",
      "|    1   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    1   N/A  N/A   1255129      C   ...IOE488/FA24/conda/rapids/bin/python        262MiB |\n",
      "|    2   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    2   N/A  N/A    938633      C   ...IOE488/FA24/conda/rapids/bin/python        422MiB |\n",
      "|    2   N/A  N/A   1255129      C   ...IOE488/FA24/conda/rapids/bin/python        262MiB |\n",
      "|    3   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    3   N/A  N/A   1255129      C   ...IOE488/FA24/conda/rapids/bin/python        262MiB |\n",
      "|    4   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    4   N/A  N/A   1255129      C   ...IOE488/FA24/conda/rapids/bin/python        262MiB |\n",
      "|    5   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    5   N/A  N/A   1255129      C   ...IOE488/FA24/conda/rapids/bin/python        262MiB |\n",
      "|    6   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    6   N/A  N/A   1255129      C   ...IOE488/FA24/conda/rapids/bin/python        262MiB |\n",
      "|    7   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    7   N/A  N/A   1255129      C   ...IOE488/FA24/conda/rapids/bin/python        262MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fd29b-e458-4c99-ab63-cb456524d5e3",
   "metadata": {},
   "source": [
    "Let's check out the connection topology of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03404cd-d681-46cd-b91a-1d1784eefcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\n",
      "GPU0\t X \tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-15\t0\t\tN/A\n",
      "GPU1\tNODE\t X \tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-15\t0\t\tN/A\n",
      "GPU2\tNODE\tNODE\t X \tNODE\tSYS\tSYS\tSYS\tSYS\t0-15\t0\t\tN/A\n",
      "GPU3\tNODE\tNODE\tNODE\t X \tSYS\tSYS\tSYS\tSYS\t0-15\t0\t\tN/A\n",
      "GPU4\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tNODE\tNODE\t16-31\t1\t\tN/A\n",
      "GPU5\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tNODE\tNODE\t16-31\t1\t\tN/A\n",
      "GPU6\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tNODE\t16-31\t1\t\tN/A\n",
      "GPU7\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\t X \t16-31\t1\t\tN/A\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c68ba-47c7-4f75-ba06-2244fc1ce936",
   "metadata": {},
   "source": [
    "And finally, let's check out the type of CPU we were allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ce24e3-22d0-4cd8-9691-132259ef2bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:        x86_64\n",
      "CPU op-mode(s):      32-bit, 64-bit\n",
      "Byte Order:          Little Endian\n",
      "CPU(s):              32\n",
      "On-line CPU(s) list: 0-31\n",
      "Thread(s) per core:  1\n",
      "Core(s) per socket:  16\n",
      "Socket(s):           2\n",
      "NUMA node(s):        2\n",
      "Vendor ID:           AuthenticAMD\n",
      "CPU family:          25\n",
      "Model:               1\n",
      "Model name:          AMD EPYC 7313 16-Core Processor\n",
      "Stepping:            1\n",
      "CPU MHz:             3000.000\n",
      "CPU max MHz:         3729.4919\n",
      "CPU min MHz:         1500.0000\n",
      "BogoMIPS:            5999.98\n",
      "Virtualization:      AMD-V\n",
      "L1d cache:           32K\n",
      "L1i cache:           32K\n",
      "L2 cache:            512K\n",
      "L3 cache:            32768K\n",
      "NUMA node0 CPU(s):   0-15\n",
      "NUMA node1 CPU(s):   16-31\n",
      "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a5376-6bd0-4d23-b61a-7a2954abab6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Introduction to CuPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbcc303-3c50-45c6-82f1-e7068e6d0665",
   "metadata": {},
   "source": [
    "NumPy is a widely used library for numerical computing in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf823dc-3fef-431f-9abb-ee827c0cf838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.6 ms ± 3.54 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "size = 512\n",
    "\n",
    "A = np.random.randn(size, size)\n",
    "\n",
    "%timeit -n 5 Q, R = np.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7dcb8c-7141-46f3-bd08-ea07261e9e65",
   "metadata": {},
   "source": [
    "CuPy uses a NumPy-like interface. Porting a Numpy code to CuPy can be as simple as changing your import statement. In this workshop, we'll always use `import cupy as cp` for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d196d0-c618-4805-863a-3b57e873d807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.6 ms ± 3.97 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "size = 512\n",
    "\n",
    "A = cp.random.randn(size, size)\n",
    "\n",
    "Q, R = cp.linalg.qr(A)\n",
    "%timeit -n 5 Q, R = cp.linalg.qr(A) ; cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954a9de-3519-400e-8a44-d59668bd26f2",
   "metadata": {},
   "source": [
    "We already see a substantial speedup with no real code changes! \n",
    "\n",
    "Notice the additional call to `cp.cuda.Device().synchronize()` in the CuPy version. GPU kernel calls are asynchronous with respect to the CPU. Our call to `synchronize()` ensures the GPU finishes to completion, so we can accurately measure  the elapsed time. We don't generally need to add these calls to production CuPy codes.\n",
    "\n",
    "NumPy is typically used to perform computations on _arrays_ of data. The data is stored in the `numpy.ndarray` object. CuPy implements a similar class called the `cupy.ndarray`. But while the `numpy.ndarray` data resides in host memory, the contents of a `cupy.ndarray` persistent in GPU memory. CuPy provides several helper functions to convert between Cupy and NumPy `ndarrays` - facilitating data transfer to/from the GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca4b14c-fe6e-4ba7-82fc-0c50219e1358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_cpu is a <class 'numpy.ndarray'>\n",
      "With initial values:\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n",
      "A_gpu is a <class 'cupy.ndarray'>\n",
      "Squared values:\n",
      " [[ 1  4  9]\n",
      " [16 25 36]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize the data on the host\n",
    "A_cpu = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
    "\n",
    "print(\"A_cpu is a\", type(A_cpu))\n",
    "print(\"With initial values:\\n\", A_cpu)\n",
    "\n",
    "#Copy data, host to device\n",
    "A_gpu = cp.asarray(A_cpu)\n",
    "print(\"A_gpu is a\", type(A_gpu))\n",
    "\n",
    "#Square the data on the device\n",
    "A_gpu = cp.square(A_gpu)\n",
    "\n",
    "#Copy data, device to host\n",
    "A_cpu = cp.asnumpy(A_gpu)\n",
    "\n",
    "print(\"Squared values:\\n\", A_cpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570d8dd-7f54-4d65-83b7-6cc38d6b4cc2",
   "metadata": {},
   "source": [
    "Note that NumPy and CuPy ndarrys are not implicitly convertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7eacf74-5f07-42b0-9b6f-8af7a302d25a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsupported type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_cpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mcupy/_core/_kernel.pyx:1286\u001b[0m, in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/_kernel.pyx:159\u001b[0m, in \u001b[0;36mcupy._core._kernel._preprocess_args\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/_kernel.pyx:145\u001b[0m, in \u001b[0;36mcupy._core._kernel._preprocess_arg\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "cp.square(A_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ba905-52c8-44af-9bea-461cd63e13c4",
   "metadata": {},
   "source": [
    "CuPy is useful for programming multi-GPU nodes as well. We can orchestrate computation, data movement, and other low-level CUDA operations with functions in the `cupy.cuda` namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d968a53d-3a98-4c2b-92b8-a780f5949d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize array on GPU 1\n",
    "with cp.cuda.Device(1):\n",
    "    A_gpu_1 = cp.array([[1, 2, 3], [4, 5, 6]], cp.int32)\n",
    "\n",
    "#Copy array from GPU 1 to GPU 0\n",
    "A_gpu_0 = cp.asarray(A_gpu_1)\n",
    "\n",
    "print(A_gpu_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc84591b-2329-44a8-a266-baa30866086b",
   "metadata": {},
   "source": [
    "The GPU is a powerhouse of parallel computing performance, and can process math operations much more quickly than the CPU. This is easy to see by comparing performance of CuPy vs NumPy, particularly for dense linear algebra operations. Let's look at a multiplication of 4096x4096 matrices. Notice the similarity of the two versions of the code (NumPy and CuPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c64595b-a9b7-43c4-a5ed-87b579aca4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Elapsed wall clock time for numpy = 0.289041 seconds.\n",
      "\n",
      "\n",
      "    Elapsed wall clock time for cupy = 0.0102659 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from time import perf_counter\n",
    "\n",
    "size = 4096\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A_cpu = np.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(np.float32)\n",
    "B_cpu = np.random.uniform(low=-1., high=1., size=(size,size) ).astype(np.float32)\n",
    "C_cpu = np.matmul(A_cpu,B_cpu)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for numpy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "del A_cpu\n",
    "del B_cpu\n",
    "del C_cpu\n",
    "\n",
    "\n",
    "\n",
    "A_gpu = cp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(cp.float32)\n",
    "B_gpu = cp.random.uniform(low=-1., high=1., size=(size,size) ).astype(cp.float32)\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu) #Exclude one-time JIT overhead\n",
    "start_time = perf_counter( )\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu)\n",
    "cp.cuda.Device(0).synchronize()\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for cupy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "del A_gpu\n",
    "del B_gpu\n",
    "del C_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9b000-e8fa-4f67-a219-4640aea54abf",
   "metadata": {},
   "source": [
    "The GPU's strenghts in computational throughput and memory bandwidth can lead to terrific application speedups. But we need to be considerate of two types of overhead when evaluating our problem for acceleration on the GPU with CuPy: kernel overhead, and data movement overhead.\n",
    "\n",
    "---\n",
    "\n",
    "**Kernel Overhead**\n",
    "\n",
    "CuPy compiles kernel codes on-the-fly using JIT compilation. Therefore, there is a compilation overhead the first time a given function is called with CuPy. The compiled kernel code is cached, so compilation overhead is avoided for subsequent executions of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae1a0d2b-da82-4e46-999c-55bbad4c0517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9313\n",
      "0.0012\n",
      "0.0011\n",
      "0.0011\n",
      "0.0011\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "size = 512\n",
    "for _ in range(5):\n",
    "    A = cp.random.randn(size, size).astype(np.float32)\n",
    "    t1 = time.time()\n",
    "    cp.linalg.det(A)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    t2 = time.time()\n",
    "    print('%.4f' % (t2 - t1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d8d9b-e4b2-4376-a441-b6bbf9a78af8",
   "metadata": {},
   "source": [
    "You may also notice a one-time overhead upon first calling a CuPy function in a program. This overhead is associated with the creation of a CUDA context by the CUDA driver, which happens the first time any CUDA API is invoked in a program.\n",
    "\n",
    "In addition, there is a CUDA kernel launch overhead that is penalized each time a GPU kernel is launched. The overhead is on the order of a few microseconds. For this reason, launching many small CUDA kernels in an application will generally lead to poor performance. The kernel launch overhead may dominate your runtime for very small problems, but for large datasets the overhead will be small compared to the actual GPU computation work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2061b6b0-169f-4e64-8556-b3fd86b97dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Matrix size: 64 x 64 \n",
      "numpy 0.000179\n",
      "cupy 0.000387\n",
      "\n",
      "Input Matrix size: 128 x 128 \n",
      "numpy 0.004947\n",
      "cupy 0.000613\n",
      "\n",
      "Input Matrix size: 256 x 256 \n",
      "numpy 0.010459\n",
      "cupy 0.000972\n",
      "\n",
      "Input Matrix size: 512 x 512 \n",
      "numpy 0.062836\n",
      "cupy 0.003190\n",
      "\n",
      "Input Matrix size: 1024 x 1024 \n",
      "numpy 0.271226\n",
      "cupy 0.008445\n",
      "\n",
      "Input Matrix size: 2048 x 2048 \n",
      "numpy 0.840295\n",
      "cupy 0.023686\n"
     ]
    }
   ],
   "source": [
    "for size in [64, 128, 256, 512, 1024, 2048]:\n",
    "    print(\"\\nInput Matrix size: %d\" % size, \"x %d \" % size)\n",
    "    for xp in [np, cp]:\n",
    "        A=xp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(xp.float32)\n",
    "        xp.linalg.qr(A)#Exclude potential one-time JIT overhead\n",
    "        t1 = time.time()\n",
    "        xp.linalg.qr(A)\n",
    "        cp.cuda.Device().synchronize()\n",
    "        t2 = time.time()\n",
    "        print(xp.__name__, '%f' % (t2 - t1))\n",
    "        del A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff335660-dfa6-4657-9489-7d31bb491d93",
   "metadata": {},
   "source": [
    "It's clear that increasing the problem size can help amoritize the overhead of launching GPU kernels. Another common strategy is to merge multiple kernels together into a single combined kernel, reducing the total number of kernel launches in your program. CuPy supports kernel fusion in this manner via the `@cupy.fuse()` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "689b1c62-62f6-4da9-a3de-3e5afd5beda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 501.76 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "2.01 ms ± 4.85 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "The slowest run took 560.63 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "997 μs ± 2.41 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "def squared_diff(x, y):\n",
    "    return (x - y) * (x - y)\n",
    "\n",
    "@cp.fuse\n",
    "def fused_squared_diff(x, y):\n",
    "    return (x - y) * (x - y)\n",
    "\n",
    "size = 10000\n",
    "\n",
    "x = cp.arange(size)\n",
    "y = cp.arange(size)[::-1]\n",
    "\n",
    "%timeit -n 10 squared_diff(x, y); cp.cuda.Device().synchronize()\n",
    "%timeit -n 10 fused_squared_diff(x, y); cp.cuda.Device().synchronize()\n",
    "\n",
    "del x\n",
    "del y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e7785-3b82-4e6e-9742-76b1caed35c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Data Movement Overhead**\n",
    "\n",
    "Try to minimize data movement to or from the GPU. The FLOP rate and memory bandwidth of a GPU can process data much more quickly than it can be fed with data over the PCIe bus. This problem is being tackled with novel interconnect technologies like NVLink. But it's a real inbalance we have to deal with for now.\n",
    "Let's look at an example where we initialize our input data GPU and then computes the dot product. Note that the result of the multiplication, the C matrix, is available on the GPU in case we need it later.\n",
    "\n",
    "Notice again the similarity of the two parts of the code (NumPy and CuPy). They are virtually identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ebb0179-8137-4827-8eb2-2c960967593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "numpy = 1.10106 seconds\n",
      "cupy = 0.140851 seconds\n",
      "Speedup = 7.82\n",
      "\n",
      "Iteration  1\n",
      "numpy = 1.10253 seconds\n",
      "cupy = 0.0395112 seconds\n",
      "Speedup = 27.90\n",
      "\n",
      "Iteration  2\n",
      "numpy = 1.0973 seconds\n",
      "cupy = 0.0383838 seconds\n",
      "Speedup = 28.59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    start_time = perf_counter( )\n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "    C_cpu = np.dot(A_cpu,B_cpu)\n",
    "    stop_time = perf_counter( )\n",
    "    cpu_time = stop_time - start_time\n",
    "    print('numpy = %g seconds' % cpu_time )\n",
    "\n",
    "    start_time = perf_counter( )\n",
    "    A_gpu=cp.random.rand(size).astype(cp.float32)\n",
    "    B_gpu=cp.random.rand(size).astype(cp.float32)\n",
    "    C_gpu = cp.dot(A_gpu,B_gpu)\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ea831-f9e3-41c1-888d-8fed77d9fd1f",
   "metadata": {},
   "source": [
    "But what if the input data for the `dot` operation resides in the system memory? We need to move the data over the PCIe bus (from the host to the GPU) using `cp.asarray()`. \n",
    "\n",
    "Modify the following cell to initialize the ndarray data with Numpy. \n",
    "\n",
    "How does the speedup change after the additional cost of data movement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be63e60-fb09-4936-886d-7e91aebacd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "cupy = 5.98002e-06 seconds\n",
      "Speedup = 183494.22\n",
      "\n",
      "Iteration  1\n",
      "cupy = 1.08965e-06 seconds\n",
      "Speedup = 1007022.58\n",
      "\n",
      "Iteration  2\n",
      "cupy = 2.11969e-06 seconds\n",
      "Speedup = 517669.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    start_time = perf_counter( )\n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "\n",
    "    start_time = perf_counter( )\n",
    "#>>>Insert CuPy code here\n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc692d-137a-44ab-8743-3a2fa3ff60bc",
   "metadata": {},
   "source": [
    "Click the `...` below to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47a42a53-e98c-4434-95ab-2114ee4d6dcd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "cupy = 1.67491 seconds\n",
      "Speedup = 0.66\n",
      "\n",
      "Iteration  1\n",
      "cupy = 1.13708 seconds\n",
      "Speedup = 0.97\n",
      "\n",
      "Iteration  2\n",
      "cupy = 1.13726 seconds\n",
      "Speedup = 0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    \n",
    "    start_time = perf_counter( )\n",
    "    \n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "    \n",
    "    A_gpu=cp.asarray(A_cpu)\n",
    "    B_gpu=cp.asarray(B_cpu)\n",
    "    C_gpu = cp.dot(A_gpu,B_gpu)\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    \n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e8fff-90ec-4af3-b453-d1a97d83c86f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Managing GPU Memory**\n",
    "\n",
    "Modern datacenter GPUs have as much as 80GB of high-bandwidth memory on a single accelerator. But in general, the host system memory will have a larger capacity. We need to be conscious of GPU memory limitations when transfering data from the host. We can query the amount of free and total memory with nvidia-smi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7af7910-1168-4bc6-bc9a-f6e887f2bd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.free [MiB], memory.total [MiB]\n",
      "3839 MiB, 49140 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -i 0 --query-gpu=memory.free,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18ee84-662a-4611-b142-0844ff46bb0b",
   "metadata": {},
   "source": [
    "Or natively with CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbbc8213-3cad-4d2b-a6dc-c345db8c1ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU (free, total) memory in bytes:\n",
      "(4024500224, 51033931776)\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU (free, total) memory in bytes:\")\n",
    "print(cp.cuda.Device().mem_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038935eb-bce2-406e-9a4e-a07b23764c08",
   "metadata": {},
   "source": [
    "Let's clear all GPU memory for good measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2e4d096-f36f-43eb-bdc7-7af50327342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU (free, total) memory in bytes:\n",
      "(5513478144, 51033931776)\n"
     ]
    }
   ],
   "source": [
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "print(\"GPU (free, total) memory in bytes:\")\n",
    "print(cp.cuda.Device().mem_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659485d-679c-49f3-81ec-4be29295fe9d",
   "metadata": {},
   "source": [
    "What happens if we try to allocate too much space on the GPU? In the following example, arrays A and B are 8GB each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e011c0b-1b76-4320-bd09-9769379c543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 32768\n",
    "A = cp.ones((size, size))\n",
    "B = cp.ones((size, size)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8d6e1-240c-4883-8462-4e1dab976ca5",
   "metadata": {},
   "source": [
    "One possible solution is to switch over to unified memory. With unified memory, the CUDA runtime will migrate data between the CPU and GPU _on demand_. Data migrations are triggered by page faults, so we may be leaving some performance on the table by using unified memory instead of managing memory explicitly. But it's an extremely convenient feature for making GPUs easier to program. We can enable Unified Memory in CuPy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c121f48-b01e-4b17-a83c-3210472ae678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a memory pool instance with malloc_managed allocator\n",
    "pool = cp.cuda.MemoryPool(cp.cuda.malloc_managed)\n",
    "cp.cuda.set_allocator(pool.malloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0a2ab-da68-49df-b712-a7cbba943c3b",
   "metadata": {},
   "source": [
    "Let's try that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf2ad44e-dcc0-475f-a085-b770dd105ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 32768\n",
    "A = cp.ones((size, size))\n",
    "B = cp.ones((size, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58382e8-2e73-488c-b318-6974deb80eba",
   "metadata": {},
   "source": [
    "We can certainly perform computations on these new arrays. Performance will take a hit as the GPU swaps pages in-and-out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e27b8daa-ef79-42f6-ac3a-7213f7fbb99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 2., ..., 2., 2., 2.],\n",
       "       [2., 2., 2., ..., 2., 2., 2.],\n",
       "       [2., 2., 2., ..., 2., 2., 2.],\n",
       "       ...,\n",
       "       [2., 2., 2., ..., 2., 2., 2.],\n",
       "       [2., 2., 2., ..., 2., 2., 2.],\n",
       "       [2., 2., 2., ..., 2., 2., 2.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.add(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82076ac4-4ec3-4913-a891-87f029760155",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Please restart the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63cbe283-6890-4758-b8b6-9d5f6d3fdfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7feab8-accd-465c-bc0a-18a677acc0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
